{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocGK8h2zEUIM"
      },
      "source": [
        "**Libraries Used** - Genism, bert-extractive-summarizer, nltk, pywsd, flashtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KYro55nPekWj",
        "outputId": "124f59e1-f088-4cec-9aff-98a7c19f55e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "2022-11-07 06:25:30.870065: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 26.8 MB/s \n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 72.5 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.14.6\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 34.5 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.1.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting joblib>=0.11\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
            "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Downloading pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 48.6 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.9 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting typing-extensions<4.2.0,>=3.7.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (806 kB)\n",
            "\u001b[K     |████████████████████████████████| 806 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.13.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.10.0-py3-none-any.whl (6.2 kB)\n",
            "Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting smart-open<6.0.0,>=5.2.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 60 kB/s \n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting click<9.0.0,>=7.1.1\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[K     |████████████████████████████████| 757 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/d8/d9/5ce0c0c91fa0d6d879a2adb5db50d45ec8704147bc88e229c46d7c5ec410/tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 25.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 40.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: zipp, typing-extensions, importlib-metadata, catalogue, urllib3, srsly, pyparsing, pydantic, numpy, murmurhash, idna, cymem, click, charset-normalizer, certifi, wasabi, typer, tqdm, smart-open, setuptools, requests, pyyaml, preshed, packaging, MarkupSafe, filelock, confection, blis, tokenizers, threadpoolctl, thinc, spacy-loggers, spacy-legacy, scipy, regex, pathy, langcodes, joblib, jinja2, huggingface-hub, transformers, spacy, scikit-learn, bert-extractive-summarizer\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.10.0\n",
            "    Uninstalling zipp-3.10.0:\n",
            "      Successfully uninstalled zipp-3.10.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.13.0\n",
            "    Uninstalling importlib-metadata-4.13.0:\n",
            "      Successfully uninstalled importlib-metadata-4.13.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.9\n",
            "    Uninstalling murmurhash-1.0.9:\n",
            "      Successfully uninstalled murmurhash-1.0.9\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.7\n",
            "    Uninstalling cymem-2.0.7:\n",
            "      Successfully uninstalled cymem-2.0.7\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.1.1\n",
            "    Uninstalling charset-normalizer-2.1.1:\n",
            "      Successfully uninstalled charset-normalizer-2.1.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.9.24\n",
            "    Uninstalling certifi-2022.9.24:\n",
            "      Successfully uninstalled certifi-2022.9.24\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.10.1\n",
            "    Uninstalling wasabi-0.10.1:\n",
            "      Successfully uninstalled wasabi-0.10.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.4.2\n",
            "    Uninstalling typer-0.4.2:\n",
            "      Successfully uninstalled typer-0.4.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 5.2.1\n",
            "    Uninstalling smart-open-5.2.1:\n",
            "      Successfully uninstalled smart-open-5.2.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: confection\n",
            "    Found existing installation: confection 0.0.3\n",
            "    Uninstalling confection-0.0.3:\n",
            "      Successfully uninstalled confection-0.0.3\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.3\n",
            "    Uninstalling spacy-loggers-1.0.3:\n",
            "      Successfully uninstalled spacy-loggers-1.0.3\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.10\n",
            "    Uninstalling spacy-legacy-3.0.10:\n",
            "      Successfully uninstalled spacy-legacy-3.0.10\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: pathy\n",
            "    Found existing installation: pathy 0.6.2\n",
            "    Uninstalling pathy-0.6.2:\n",
            "      Successfully uninstalled pathy-0.6.2\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.3.0\n",
            "    Uninstalling langcodes-3.3.0:\n",
            "      Successfully uninstalled langcodes-3.3.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.2.0\n",
            "    Uninstalling joblib-1.2.0:\n",
            "      Successfully uninstalled joblib-1.2.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed MarkupSafe-2.1.1 bert-extractive-summarizer-0.10.1 blis-0.7.9 catalogue-2.0.8 certifi-2022.9.24 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 cymem-2.0.7 filelock-3.8.0 huggingface-hub-0.10.1 idna-3.4 importlib-metadata-5.0.0 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 numpy-1.21.6 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 scikit-learn-1.0.2 scipy-1.7.3 setuptools-65.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tokenizers-0.13.1 tqdm-4.64.1 transformers-4.24.0 typer-0.4.2 typing-extensions-4.1.1 urllib3-1.26.12 wasabi-0.10.1 zipp-3.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.1.3\n",
            "  Downloading spacy-2.1.3-cp37-cp37m-manylinux1_x86_64.whl (27.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Collecting wasabi<1.1.0,>=0.2.0\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0\n",
            "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting jsonschema<3.0.0,>=2.6.0\n",
            "  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Using cached murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting numpy>=1.15.0\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting srsly<1.1.0,>=0.0.5\n",
            "  Downloading srsly-1.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 299 kB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Using cached cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 59.3 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "Collecting charset-normalizer<3,>=2\n",
            "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "Collecting tqdm<5.0.0,>=4.10.0\n",
            "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Installing collected packages: numpy, cymem, wasabi, urllib3, tqdm, srsly, preshed, plac, murmurhash, idna, charset-normalizer, certifi, blis, thinc, requests, jsonschema, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.7\n",
            "    Uninstalling cymem-2.0.7:\n",
            "      Successfully uninstalled cymem-2.0.7\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.10.1\n",
            "    Uninstalling wasabi-0.10.1:\n",
            "      Successfully uninstalled wasabi-0.10.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.12\n",
            "    Uninstalling urllib3-1.26.12:\n",
            "      Successfully uninstalled urllib3-1.26.12\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.9\n",
            "    Uninstalling murmurhash-1.0.9:\n",
            "      Successfully uninstalled murmurhash-1.0.9\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.1.1\n",
            "    Uninstalling charset-normalizer-2.1.1:\n",
            "      Successfully uninstalled charset-normalizer-2.1.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.9.24\n",
            "    Uninstalling certifi-2022.9.24:\n",
            "      Successfully uninstalled certifi-2022.9.24\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.1\n",
            "    Uninstalling requests-2.28.1:\n",
            "      Successfully uninstalled requests-2.28.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.1.3 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\n",
            "altair 4.2.0 requires jsonschema>=3.0, but you have jsonschema 2.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 certifi-2022.9.24 charset-normalizer-2.1.1 cymem-2.0.7 idna-3.4 jsonschema-2.6.0 murmurhash-1.0.9 numpy-1.21.6 plac-0.9.6 preshed-2.0.1 requests-2.28.1 spacy-2.1.3 srsly-1.0.6 thinc-7.0.8 tqdm-4.64.1 urllib3-1.26.12 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click->nltk) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pywsd\n",
            "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 110.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pywsd) (3.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.21.6)\n",
            "Collecting wn==0.0.23\n",
            "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click->nltk->pywsd) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk->pywsd) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk->pywsd) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2022.5)\n",
            "Building wheels for collected packages: wn\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792911 sha256=9f1c40b19d560ff0111284bcb35c5866f050dc5453f95dfef3cf4e1d4423ff31\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/47/17/409766c99dd470f34c512000b90b83f34747c2c975769654d7\n",
            "Successfully built wn\n",
            "Installing collected packages: wn, pywsd\n",
            "Successfully installed pywsd-1.2.5 wn-0.0.23\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.24.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=8e9b13c1f291aed45f0cde13f3cddabc5194e3f5607f5cf60ccc71258507513b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-5ppywjmw\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-5ppywjmw\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.7.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.2.0)\n",
            "Collecting spacy>=3.2.3\n",
            "  Using cached spacy-3.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Using cached thinc-8.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (806 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.10.2)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (65.5.1)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Using cached srsly-2.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.3->pke==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.3->pke==2.0.0) (5.2.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.0.3)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Using cached blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (5.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pke==2.0.0) (2022.10.31)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==2.0.0) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pke==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: pke, sklearn\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160264 sha256=e08664196e2fa775b0ff1d7100afc25c65d2793c4ba34b5959b36d559eb150e8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1wjl3fm2/wheels/fa/b3/09/612ee93bf3ee4164bcd5783e742942cdfc892a86039d3e0a33\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=e88ff9798916b3e73e877f879a52c03401fc142c81f15f5301d3842db409b86d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built pke sklearn\n",
            "Installing collected packages: srsly, preshed, blis, thinc, unidecode, spacy, sklearn, pke\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.6\n",
            "    Uninstalling srsly-1.0.6:\n",
            "      Successfully uninstalled srsly-1.0.6\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.1.3\n",
            "    Uninstalling spacy-2.1.3:\n",
            "      Successfully uninstalled spacy-2.1.3\n",
            "Successfully installed blis-0.7.9 pke-2.0.0 preshed-3.0.8 sklearn-0.0 spacy-3.4.2 srsly-2.4.5 thinc-8.1.5 unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9298 sha256=1e502b24fe865f8eeb56e25708a37e229e5b39bc5596804d918646c0ffac5f85\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!python -m spacy download en\n",
        "!pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
        "!pip install spacy==2.1.3 --upgrade --force-reinstall\n",
        "!pip install -U nltk\n",
        "!pip install -U pywsd\n",
        "!pip install -U sentence-transformers\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install flashtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing nltk libraries : wordnet and popular"
      ],
      "metadata": {
        "id": "Zy72qdlN3RHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqFos45qevVS",
        "outputId": "d39681bc-b1b4-41b6-907b-9fcdfa94f969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarizing the input file using Summarizer()"
      ],
      "metadata": {
        "id": "Icdc6Fv56BaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aos7mQ4vewMF",
        "outputId": "88e4b457-d017-4f9c-8284-6531c5f06116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from summarizer import Summarizer\n",
        "\n",
        "f = open(\"Test4.txt\",\"r\",encoding=\"utf8\")\n",
        "full_text = f.read()\n",
        "\n",
        "model = Summarizer()\n",
        "result = model(full_text, min_length=60, max_length = 1000 , ratio = 0.2)\n",
        "\n",
        "summarized_text = ''.join(result)\n",
        "#print (summarized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting keywords in the summarised text"
      ],
      "metadata": {
        "id": "a8D0xDhE6T6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9xAzzEfewOm",
        "outputId": "c52395d9-8dcc-4851-aa6c-ebcf74a1c360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['ner', 'textcat', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "import itertools\n",
        "import re\n",
        "import pke\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def get_nouns_multipartite(text):\n",
        "    out=[]\n",
        "\n",
        "    extractor = pke.unsupervised.MultipartiteRank()\n",
        "    #print(extractor)\n",
        "    \n",
        "    # not contain punctuation marks or stopwords as candidates.\n",
        "    pos = {'PROPN'}\n",
        "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
        "    stoplist = list(string.punctuation)\n",
        "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "    #stoplist += stopwords.words('english')\n",
        "    stoplist += pke.lang.stopwords.get('en')\n",
        "\n",
        "    extractor.load_document(input=text,stoplist = stoplist)\n",
        "    extractor.candidate_selection( pos=pos)\n",
        "    # build the Multipartite graph and rank candidates using random walk,\n",
        "    # alpha controls the weight adjustment mechanism.\n",
        "    extractor.candidate_weighting(alpha=1.1,\n",
        "                                  threshold=0.75,\n",
        "                                  method='average')\n",
        "    keyphrases = extractor.get_n_best(n=500)\n",
        "\n",
        "    for key in keyphrases:\n",
        "        out.append(key[0])\n",
        "\n",
        "    return out\n",
        "\n",
        "keywords = get_nouns_multipartite(full_text) \n",
        "#print(\"Number of keywords = \", len(keywords))\n",
        "#print (keywords)\n",
        "\n",
        "filtered_keys=[]\n",
        "for keyword in keywords:\n",
        "    if keyword.lower() in summarized_text.lower():\n",
        "        filtered_keys.append(keyword)\n",
        "#print(\"Number of keywords in the summarised text = \", len(keyword))        \n",
        "#print (filtered_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenisation and Mapping with the extracted keywords above"
      ],
      "metadata": {
        "id": "rWTEp2lb6gx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mvNFJ-VewQ0",
        "outputId": "e89da301-5f65-4263-f464-cefe5c73d0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'definition': ['Some terms can be used to designate the\\nstate of a node:\\nDefinition 1 (Unknown Nodes — U): Also\\nknown as free or dumb nodes, this term refers to\\nthe nodes of the network that do not know their\\nlocalization information.'], 'fig': ['When new position information is received from other nodes, it becomes\\npossible to identify the probable location of the\\nunknown node, as depicted in Fig.', 'The RPE algorithm can be divided into four\\nphases, as depicted in Fig.'], 'localization problem': ['The necessity of different solutions for different applications\\nand also the high number of possible applications of WSNs have greatly motivated the study\\nand proposals of new solutions to the localization problem.', '[14] who formulate the localization problem as a\\nconvex optimization problem based only on connectivity-induced constraints and use a semidefinite program (SDP) to solve the problem.', 'These components can be seen as\\nsubareas of the localization problem that need\\nto be studied separately.', 'FINAL REMARKS\\nThis article has addressed the localization problem from the viewpoint of a WSN.'], 'node': ['The circles formed by the position and distance to each of the references can be represented by the formula (^x – xi)2 + (^y – yi)2 = di\\n2,\\nwhere(^x , ^y ) is the position we want to compute,\\n(xi, yi) is the position of the ith reference node,\\nand di is the distance of the ith reference node\\nto the unknown node.', 'Location fingerprinting is a method in which the signal characteristics\\nobtained from a set of locations are catalogued,\\nand the position computation of a node consists of\\ncomparing its current signal characteristics with\\nthose catalogued previously.', 'The\\nunknown node estimates its angle to each of the\\nthree reference nodes and, based on these angles\\nand the positions of the reference nodes (which\\nform a triangle), computes its own position using\\nsimple trigonometrical relationships.', 'Some terms can be used to designate the\\nstate of a node:\\nDefinition 1 (Unknown Nodes — U): Also\\nknown as free or dumb nodes, this term refers to\\nthe nodes of the network that do not know their\\nlocalization information.', 'in the\\nAPIT algorithm, use triangles formed by three\\nbeacon nodes, and a node decides if it is inside or\\noutside these triangles by comparing its signal\\nstrength measurements with the measurements of\\nits neighbors.', 'When new position information is received from other nodes, it becomes\\npossible to identify the probable location of the\\nunknown node, as depicted in Fig.', 'When an unknown\\nnode receives a packet from a reference node, it\\ncan be in any place around the reference node\\nwith equal probabilities.', 'When an unknown\\nnode receives a packet from a reference node, it\\ncan be in any place around the reference node\\nwith equal probabilities.', 'When an unknown\\nnode receives a packet from a reference node, it\\ncan be in any place around the reference node\\nwith equal probabilities.', 'The position of the node is computed by finding the centroid of the intersection of\\nthe beacon triangles the node is within.', 'The position of the node is computed by finding the centroid of the intersection of\\nthe beacon triangles the node is within.', 'In the final phase the node becomes a\\nreference node by broadcasting its newly estimated position to its neighbors.', 'In the final phase the node becomes a\\nreference node by broadcasting its newly estimated position to its neighbors.', 'COMMENTS ABOUT POSITION COMPUTATION\\nA number of other methods exist that aim to compute the position of a node.', 'When a node\\nbecomes a reference, it can assist other nodes in\\ncomputing their positions as well.'], 'beacon nodes': ['in the\\nAPIT algorithm, use triangles formed by three\\nbeacon nodes, and a node decides if it is inside or\\noutside these triangles by comparing its signal\\nstrength measurements with the measurements of\\nits neighbors.'], 'localization': ['However, the way distances are propagated, especially in Dv-Hop and\\nDv-Distance, as well as the way these distances\\nare converted from hops to meters in Dv-Hop,\\nresult in erroneous position computation, which\\nincreases the final localization error of the system.', 'Localization algorithms can be classified into\\na few categories: distributed  or centralized position computation ; with\\nor without an infrastructure; relative\\nor absolute positioning ; designed\\nfor indoor or outdoor scenarios; and\\none hop  or multihop .', 'Some terms can be used to designate the\\nstate of a node:\\nDefinition 1 (Unknown Nodes — U): Also\\nknown as free or dumb nodes, this term refers to\\nthe nodes of the network that do not know their\\nlocalization information.', 'COMMENTS ABOUT THE\\nDISTANCE/ANGLE ESTIMATION\\nThe choice of which method to use to estimate\\nthe distance between nodes in a localization system is an important factor that influences the\\nfinal performance of the system.', 'Here some proposed localization algorithms\\nare discussed to show how this component differentiates from the other components.', 'LOCALIZATION ALGORITHM\\nThe localization algorithm is the main component of a localization system.', 'To allow these nodes to\\nestimate their positions is the main goal of a\\nlocalization system.'], 'angle': ['The\\nunknown node estimates its angle to each of the\\nthree reference nodes and, based on these angles\\nand the positions of the reference nodes (which\\nform a triangle), computes its own position using\\nsimple trigonometrical relationships.', 'Distance/angle estimation consists in identifying\\nthe distance or angle between two nodes.'], 'arrival': ['TIME [DIFFERENCE] OF ARRIVAL\\nDifferent methods try to estimate distances\\nbetween two nodes using time based measures.'], 'rpe': ['The RPE algorithm can be divided into four\\nphases, as depicted in Fig.'], 'algorithm': ['in the\\nAPIT algorithm, use triangles formed by three\\nbeacon nodes, and a node decides if it is inside or\\noutside these triangles by comparing its signal\\nstrength measurements with the measurements of\\nits neighbors.', 'The RPE algorithm can be divided into four\\nphases, as depicted in Fig.'], 'received': ['When new position information is received from other nodes, it becomes\\npossible to identify the probable location of the\\nunknown node, as depicted in Fig.'], 'distance': ['The circles formed by the position and distance to each of the references can be represented by the formula (^x – xi)2 + (^y – yi)2 = di\\n2,\\nwhere(^x , ^y ) is the position we want to compute,\\n(xi, yi) is the position of the ith reference node,\\nand di is the distance of the ith reference node\\nto the unknown node.', 'The circles formed by the position and distance to each of the references can be represented by the formula (^x – xi)2 + (^y – yi)2 = di\\n2,\\nwhere(^x , ^y ) is the position we want to compute,\\n(xi, yi) is the position of the ith reference node,\\nand di is the distance of the ith reference node\\nto the unknown node.', 'However, the way distances are propagated, especially in Dv-Hop and\\nDv-Distance, as well as the way these distances\\nare converted from hops to meters in Dv-Hop,\\nresult in erroneous position computation, which\\nincreases the final localization error of the system.', 'COMMENTS ABOUT THE\\nDISTANCE/ANGLE ESTIMATION\\nThe choice of which method to use to estimate\\nthe distance between nodes in a localization system is an important factor that influences the\\nfinal performance of the system.', 'COMMENTS ABOUT THE\\nDISTANCE/ANGLE ESTIMATION\\nThe choice of which method to use to estimate\\nthe distance between nodes in a localization system is an important factor that influences the\\nfinal performance of the system.', 'In real-world applications the distance estimation inaccuracies as well as the inaccurate\\nposition information of reference nodes make it\\ndifficult to compute a position.', 'We divided\\nlocalization systems into three components: distance/angle estimation, position computation,\\nand localization algorithm.', 'Distance/angle estimation consists in identifying\\nthe distance or angle between two nodes.', 'Distance/angle estimation consists in identifying\\nthe distance or angle between two nodes.'], 'unknown nodes': ['Some terms can be used to designate the\\nstate of a node:\\nDefinition 1 (Unknown Nodes — U): Also\\nknown as free or dumb nodes, this term refers to\\nthe nodes of the network that do not know their\\nlocalization information.'], 'trilateration': ['Despite the final error of this method, which\\nis greater than trilateration, computing the intersection of squares uses fewer processor resources\\nthan computing the intersection of circles.'], 'dv-hop': ['However, the way distances are propagated, especially in Dv-Hop and\\nDv-Distance, as well as the way these distances\\nare converted from hops to meters in Dv-Hop,\\nresult in erroneous position computation, which\\nincreases the final localization error of the system.', 'However, the way distances are propagated, especially in Dv-Hop and\\nDv-Distance, as well as the way these distances\\nare converted from hops to meters in Dv-Hop,\\nresult in erroneous position computation, which\\nincreases the final localization error of the system.'], 'localization algorithm': ['We divided\\nlocalization systems into three components: distance/angle estimation, position computation,\\nand localization algorithm.', 'LOCALIZATION ALGORITHM\\nThe localization algorithm is the main component of a localization system.', 'LOCALIZATION ALGORITHM\\nThe localization algorithm is the main component of a localization system.'], 'sdp': ['[14] who formulate the localization problem as a\\nconvex optimization problem based only on connectivity-induced constraints and use a semidefinite program (SDP) to solve the problem.'], 'convex': ['[14] who formulate the localization problem as a\\nconvex optimization problem based only on connectivity-induced constraints and use a semidefinite program (SDP) to solve the problem.'], 'angle estimation': ['COMMENTS ABOUT THE\\nDISTANCE/ANGLE ESTIMATION\\nThe choice of which method to use to estimate\\nthe distance between nodes in a localization system is an important factor that influences the\\nfinal performance of the system.', 'We divided\\nlocalization systems into three components: distance/angle estimation, position computation,\\nand localization algorithm.', 'Distance/angle estimation consists in identifying\\nthe distance or angle between two nodes.'], 'beacon': ['The position of the node is computed by finding the centroid of the intersection of\\nthe beacon triangles the node is within.', 'This beacon can be a\\nhuman operator, an unmanned vehicle, an aircraft, or a robot.'], 'location fingerprinting': ['Location fingerprinting is a method in which the signal characteristics\\nobtained from a set of locations are catalogued,\\nand the position computation of a node consists of\\ncomparing its current signal characteristics with\\nthose catalogued previously.'], 'apit': ['in the\\nAPIT algorithm, use triangles formed by three\\nbeacon nodes, and a node decides if it is inside or\\noutside these triangles by comparing its signal\\nstrength measurements with the measurements of\\nits neighbors.'], 'wsn': ['This component\\ndetermines how the information concerning distances and positions is manipulated in order to\\nallow most or all of the nodes of a WSN to estimate their positions.', 'FINAL REMARKS\\nThis article has addressed the localization problem from the viewpoint of a WSN.'], 'localization systems': ['and other indoor localization systems, but the need to generate a signal signature database makes this technique unfeasible\\nfor most scenarios of WSNs.', 'We divided\\nlocalization systems into three components: distance/angle estimation, position computation,\\nand localization algorithm.', 'This article has shown a number of proposed\\nlocalization systems, each with an emphasis on a\\nspecific scenario and/or application.', 'In the following sections some of the main\\nmethods used by localization systems to estimate\\ndistances/angles will be studied.', 'We show the main methods used by localization systems to estimate distances and angles.'], 'ith reference node': ['The circles formed by the position and distance to each of the references can be represented by the formula (^x – xi)2 + (^y – yi)2 = di\\n2,\\nwhere(^x , ^y ) is the position we want to compute,\\n(xi, yi) is the position of the ith reference node,\\nand di is the distance of the ith reference node\\nto the unknown node.', 'The circles formed by the position and distance to each of the references can be represented by the formula (^x – xi)2 + (^y – yi)2 = di\\n2,\\nwhere(^x , ^y ) is the position we want to compute,\\n(xi, yi) is the position of the ith reference node,\\nand di is the distance of the ith reference node\\nto the unknown node.'], 'computation': ['However, the way distances are propagated, especially in Dv-Hop and\\nDv-Distance, as well as the way these distances\\nare converted from hops to meters in Dv-Hop,\\nresult in erroneous position computation, which\\nincreases the final localization error of the system.', 'Localization algorithms can be classified into\\na few categories: distributed  or centralized position computation ; with\\nor without an infrastructure; relative\\nor absolute positioning ; designed\\nfor indoor or outdoor scenarios; and\\none hop  or multihop .', 'Location fingerprinting is a method in which the signal characteristics\\nobtained from a set of locations are catalogued,\\nand the position computation of a node consists of\\ncomparing its current signal characteristics with\\nthose catalogued previously.', 'We divided\\nlocalization systems into three components: distance/angle estimation, position computation,\\nand localization algorithm.', 'COMMENTS ABOUT POSITION COMPUTATION\\nA number of other methods exist that aim to compute the position of a node.'], 'final remarks': ['FINAL REMARKS\\nThis article has addressed the localization problem from the viewpoint of a WSN.']}\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from flashtext import KeywordProcessor\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = [sent_tokenize(text)]\n",
        "    sentences = [y for x in sentences for y in x]\n",
        "    # Remove any short sentences less than 10 letters.\n",
        "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 10]\n",
        "    return sentences\n",
        "\n",
        "def get_sentences_for_keyword(keywords, sentences):\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    keyword_sentences = {}\n",
        "    for word in keywords:\n",
        "        keyword_sentences[word] = []\n",
        "        keyword_processor.add_keyword(word)\n",
        "    for sentence in sentences:\n",
        "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
        "        for key in keywords_found:\n",
        "            keyword_sentences[key].append(sentence)\n",
        "\n",
        "    for key in keyword_sentences.keys():\n",
        "        values = keyword_sentences[key]\n",
        "        values = sorted(values, key=len, reverse=True)\n",
        "        keyword_sentences[key] = values\n",
        "    return keyword_sentences\n",
        "\n",
        "sentences = tokenize_sentences(summarized_text)\n",
        "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
        "        \n",
        "print (keyword_sentence_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating distractors using Wordnet and conceptnet and storing the Mcqs in a text file."
      ],
      "metadata": {
        "id": "85khyK-e6phS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMofr5QFewS-",
        "outputId": "ca45bf15-5237-4caf-d815-94008baae870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up PyWSD (takes ~10 secs)... took 8.812906980514526 secs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('definition.n.01')\n",
            "Synset('libyan_islamic_fighting_group.n.01')\n",
            "Synset('node.n.04')\n",
            "Synset('localization.n.01')\n",
            "Synset('angle.n.01')\n",
            "Synset('arrival.n.02')\n",
            "Synset('algorithm.n.01')\n",
            "Synset('distance.n.02')\n",
            "Synset('radio_beacon.n.02')\n",
            "Synset('calculation.n.01')\n",
            "\n",
            "More options:  ['Exposition', 'Gloss', 'Interpretation', 'Justification', 'Reason', 'Simplification', 'Walk-through'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Blind Spot', 'Celestial Point', 'Center', 'Chokepoint', 'Corner', 'Crossing', 'Focus', 'Geographic Point', 'Ground Zero', 'Hilum', 'Hot Spot', \"Mcburney's Point\", 'Midair', 'Military Position', 'Navel', 'Position'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Rectification', 'Redetermination', 'Resolution', 'Validation'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Enclosure', 'Expanse', 'Hole', 'Opening', 'Pleural Space', 'Pocket', 'Subarachnoid Space', 'Swath', 'Void'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Application', 'Beatification', 'Bruxism', 'Change', 'Choice', 'Course', 'Destabilization', 'Economy', 'Emphasizing', 'Employment', 'Fetch', 'Interaction', 'Jumpstart', 'Kindness', 'Performance', 'Pickings'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Belt', 'Biosphere', 'Black Hole', 'Bottom', 'County', 'Deep Space', 'Depth', 'Eden', 'Extremity', 'Heliosphere', 'Hell', 'Inside', 'Intergalactic Space', 'Interplanetary Space', 'Interstellar Space', 'Ionosphere'] \n",
            "\n",
            "\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n",
            "\n",
            "More options:  ['Diagnostic Procedure', 'Emergency Procedure', 'Experimental Procedure', 'Fingerprinting', 'Genetic Profiling', 'Indirection', 'Mapping', 'Medical Procedure', 'Operating Procedure', 'Rigmarole', 'Routine', 'Rule', 'Stiffening'] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from pywsd.similarity import max_similarity\n",
        "from pywsd.lesk import adapted_lesk\n",
        "from pywsd.lesk import simple_lesk\n",
        "from pywsd.lesk import cosine_lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "\n",
        "def get_wordsense(sent,word):\n",
        "    word= word.lower()\n",
        "    #print(word.split())\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    \n",
        "    synsets = wn.synsets(word,'n')\n",
        "    if synsets:\n",
        "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
        "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
        "        #finding similar words using max_similarity and adapted lesk and comparing them\n",
        "        #print(synsets.index(wup))\n",
        "        #print(synsets.index(adapted_lesk_output))\n",
        "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
        "        return synsets[lowest_index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Distractors from Wordnet\n",
        "def get_distractors_wordnet(syn,word):\n",
        "    print(syn)\n",
        "    distractors=[]\n",
        "    word= word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0: \n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        #print (\"name \",name, \" word\",orig_word)\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\",\" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "\n",
        "\n",
        "# Distractors from http://conceptnet.io/\n",
        "def get_distractors_conceptnet(word):\n",
        "    word = word.lower()\n",
        "    original_word= word\n",
        "    if (len(word.split())>0):\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    distractor_list = [] \n",
        "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
        "    obj = requests.get(url).json()\n",
        "\n",
        "    for edge in obj['edges']:\n",
        "        link = edge['end']['term'] \n",
        "\n",
        "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
        "        obj2 = requests.get(url2).json()\n",
        "        for edge in obj2['edges']:\n",
        "            word2 = edge['start']['label']\n",
        "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
        "                distractor_list.append(word2)\n",
        "                   \n",
        "    return distractor_list\n",
        "\n",
        "\n",
        "key_distractor_list = {}\n",
        "#print(keyword_sentence_mapping)\n",
        "for keyword in keyword_sentence_mapping:\n",
        "  #function defined above - to get the question for the keyword found\n",
        "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
        "    #print(wordsense)\n",
        "    #if similar word exists then perform following:\n",
        "    if wordsense:\n",
        "        #Distractors are the wrong answers in a multiple-choice question.\n",
        "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
        "        #print(distractors)\n",
        "        if len(distractors) ==0:\n",
        "            distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors\n",
        "    else:\n",
        "        \n",
        "        distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors\n",
        "\n",
        "index = 1\n",
        "\n",
        "file1 = open(\"MCQS.txt\", \"w\")\n",
        "\n",
        "for each in key_distractor_list:\n",
        "    sentence = keyword_sentence_mapping[each][0]\n",
        "    pattern = re.compile(each, re.IGNORECASE)\n",
        "    output = \"\\n\"+ str(index) + \":\" + pattern.sub( \" _______ \", sentence) +\"\\n\"\n",
        "    file1.writelines(output)\n",
        "    \n",
        "    #print (\"%s)\"%(index),output)\n",
        "    choices = [each.capitalize()] + key_distractor_list[each]\n",
        "    top4choices = choices[:4]\n",
        "    random.shuffle(top4choices)\n",
        "    optionchoices = ['a','b','c','d']\n",
        "    for idx,choice in enumerate(top4choices):\n",
        "        mcqs = optionchoices[idx]+\":\"+choice+\"\\n\"\n",
        "        #print (mcqs)\n",
        "        file1.writelines(mcqs)\n",
        "        \n",
        "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
        "    index = index + 1\n",
        "\n",
        "file1.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}